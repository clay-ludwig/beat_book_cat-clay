## Beat book process for beginners		02/14/2026

Before we proceed with this task, users should:
    -Have story samples in a json file
    -Have a Github account
If these requirements are met, the next thing is to set up our environment. Note: we will be using Groq models for this process, but users can feel free to install other models.
## Quick setup for beginners
First create a repository: 
1. Go to github.com -> Log into your account
	- Click the + icon in the top-right, choose New repository.
	- Name it, set Public or Private, then click Create repository.
2. Open your codespace or terminal:
	- Click Code -> Codespaces -> Create codespace.
	- We'll work locally, so open a terminal in your project folder. [explain this like they have never used a terminal, explain parts of the terminal. keep it concise but detailed. Let the user know how to install copilot]
3. Install uv (Python tool runner):
	- Run: curl -LsSf https://astral.sh/uv/install.sh | sh
	- Then run: export PATH="$HOME/.local/bin:$PATH"
4. Initialize your project (optional but helpful):
	- Run: uv init --python 3.12
5. Install llm:
	- Run: uv add llm
    - Run: uv add llm install llm-groq (Here, you can include other models: llm-gemini, llm-anthropic, especially if you have access to their API keys).
6. Get a Groq API key:
	- Go to https://console.groq.com
	- Create an account and an API key (click on Create API at the top right corner when you log in to Groq).
    - To set up your Groq API key which should help you access the models, run: uv run llm keys set groq. You'll be prompted to enter your API keys. Bear in mind that you wont see the key, but when you click 'enter/return' it should work.

## Next steps: Extract entities
Now that we are all set up, our next step is to begin gathering/extracting entities: metadata from the stories we have. One reason for extracting entities is that it influences how our beatbooks turn out. A chronological beat book for instance may not turn out well if this the format is not taken into consideration at this stage [Explain further, do not oversimpify.]
We can use any of the models we have access to to extract metadata. We also need a python script for this process, but since many people are not versed in Python, we'll turn to copilot to generate a script that extracts our metadata.
Although the beatbook is the main product, this is a particularly important step. It can make or mar the product [talk a little more about this]
But while metadata can provide [what ?] it is not enough context for a beatbook.
To get more context for a beatbook, we need to have some parts of our story included in the beatbook script. We can choose to include entire stories as they appear in our original data, or just summaries. Summaries work best because [conserve token: get results while feeding in less info]

I decided I wanted a chronologically organized beatbook that would look at events over a period of time.
I generated my entities extraction script by given copilot the folllowing instuctions:

We want to extract entities from public_safety_stories.json. Write me a script to extract entities for a thematic beatbook that looks at public safety thematically over time. For example, common issues that occur in the summer vs. the winter, etc. It should: extract the entities first, then summarize the stories in the public_safety_stories.json. Also, it shoudl retain all quotes in the stories, and replace the story content with the summaries. I want metadata fields to include:
Temporal information:
names: (important people mentioned in the stories over time)
season (winter/spring/summer/fall - can be derived)
year
is_weekend (boolean)

Content Classiification:
primary_theme (e.g., "traffic accidents", "violent crime", "fire/rescue")
secondary_themes (list - articles often cover multiple issues)
incident_type (more specific: "pedestrian fatality", "armed robbery", "house fire")
severity_level (e.g., "minor", "moderate", "major" - based on injuries, damage,response)

Geographic Information:
location (neighborhood/district)
location_type (residential, commercial, highway, park, school zone)

Contextual Details:
time_of_incident (if mentioned - helps identify patterns like late-night crimes)
weather_conditions (if relevant/mentioned)
response_agencies (police, fire, EMS, multiple)
outcome (arrest made, under investigation, resolved, ongoing)

PRO TIP: When unsure about what directions to give your AI tool, ask AI!
[SAMPLE PROMPT: I'd like to generate metadata from articles that will help me create a beat book that looks at public safety issues thematically over time. For example, common issues that occur in the summer vs. the winter. What would the structure of that metadata need to look like?]

 As I examined the script AI generated, I included other requirements: 
 	Metadata be extracted from the original stories first, before they are summarized. I did not want to be limited to only the metadata found in the summaries.
	- Output: replace full story content with the summary text in the saved JSON.
	Exclusions in metadata extraction:
	- Do not include author/byline names.
	- Do not include photographer names.
	- Do not include news organization names (Star Democrat, Chesapeake Publishing, APG Media).
	Incremental saves:
	- The script writes the output file after each story is processed.
[talk about rate limits: Incremental saves are important because you are likely to hit rate limits because of ??]
[to get through this, one needs to update the .py script that extract the eentities to save incrementally. That way, when you run into a problem, you can continue saving from where you stopped. Sometimes you have too many stories. It's best to process them in batches. Or how best to deal with this?]
[It's usually best to use closed models: why?]
[after generating entities, you want to study it to see what your extracted values look like. SQLite database is a good way to do that because?]
First you laod your json file into a sqlite database using this command: 
`uv run sqlite-utils insert entities.db stories your_json_file_name --pk docref`

Replace 'your_json_file_name' with the name of the json file where you have the entities. The command above converts your json file into a database table that lives in entities.db. Now you can filter, analyze the data using datasette.
What is datasette? Datasette is [ ??]
To view your document on datasette you first have to install it in your environment: 
`uv add datasette`
You know you added it correctly when your output says: "Installed 15 packages in 58ms" and goes ahead to list the packages installed.
Then you can run this to view your data in Datasette:
`uv run datasette entities.db`
What the command above does is take you to Datasette on you browser. Running this command prompts a pop up that asks you if you want to open datasette in a browser. If, somehow, you miss the pop up or close it, you can access datasette by clicking on 'PORTS' locted at the top of your terminal. You should look for Port 8001, and you know this is what you are looking for because in your terminal, the last line there says: Uvicorn running on http://127.0.0.1:8001
The last four figures there tells you what port to look for.
CLick on the link. A globe icon pops up by the side. Click on that and look through your data. Your data is linked to "stories" under entities [we know this how?]
Looking at your work in datasette lets you see how well your extraction worked. It also helps you with factchecking: are there false names in the extracted data? You are able to compare the extractions with the summary you have. You also get to see how many cases your llm failed to parse or extract your data, which gives a sense of how reliable the data is.
If you are not content with your extracted entities, you can review the process afresh, prompting copilot to update the script so that it reflects whatever you wish to include or take out.
If you are unsure of what to do to update your script, describe what is wrong to copilot or any other llm and have them make suggestions for you.
This step requires some refinement and should be carried out multiple times.
There is no need to rush through this.
You may choose to create your beatbook in a way that focuses on the most important people mentioned, or the most significant issues covered in one beat. ANYTHING can infuence the angle from which you may choose to create your beatbook. I have decided to create a beatbook that is chronological narrative? This helps a new reporter not only understand the beat, but see how that has evolved over time.
One other significant factor to consider when creating a beatbook is the format. Will this be in text? A table? A webpage?

## Writing the beat book
This process will largely be handled by your AI tool (preferably a commercial model?). Again, what this means is that we need a Python script to [do what?]
To generate this, we go again to copilot and ask it to generate a script.
Many people have limited knowledge of Python, but it is important to pay attention to this script, especially to the areas that are not code, which you can understand. Understand the sections of your script that has your prompt in it and go through them to be sure they are in line with what you [need out of your beat book]. You may ask copilot to do a break down of the script, so that you understand it better. It helps you provide directions in your script so that even when you are unsure how the code works, you can direct the AI throught the prompts you provide in plain language.

uv run python generate_thematic_beatbook.py \
  --model "groq/llama-3.3-70b-versatile" \
  --input thematic_entities_stories.json \
  --output my_beatbook.md